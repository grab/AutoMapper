{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T07:56:53.097508Z",
     "start_time": "2025-06-23T07:56:52.856401Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.wkt import loads\n",
    "from enum import Enum\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Issues Investigation Notebook\n",
    "\n",
    "This notebook helps you **visually investigate evaluation results** on an interactive map.\n",
    "\n",
    "**Prerequisites:** Run `eval.py` first to generate issues files.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads pre-computed issues files from `evaluation_results/`\n",
    "2. Filters issues by OSM tag (name, oneway, lanes, etc.)\n",
    "3. Visualizes issues on an interactive map with color coding\n",
    "4. Allows drilling down into specific issue types (FP, FN, Mismatch)\n",
    "5. Supports comparing multiple models\n",
    "\n",
    "**Note:** This notebook does NOT re-run evaluation - it uses the results from `eval.py`.\n",
    "\n",
    "---\n",
    "\n",
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_FEATURES = ['name',\n",
    "                'oneway',\n",
    "                'turn:lanes',\n",
    "                'turn:lanes:forward',\n",
    "                'turn:lanes:backward',\n",
    "                'lanes',\n",
    "                'lanes:forward',\n",
    "                'lanes:backward',\n",
    "                'maxspeed',\n",
    "                'maxspeed:forward',\n",
    "                'maxspeed:backward']\n",
    "\n",
    "class MapFeatures(Enum):\n",
    "    STREET_NAME = \"name\"\n",
    "    ONEWAY = \"oneway\"\n",
    "    TURN_LANES = \"turn:lanes\"\n",
    "    TURN_LANES_FWD = \"turn:lanes:forward\"\n",
    "    TURN_LANES_BWD = \"turn:lanes:backward\"\n",
    "    LANES = \"lanes\"\n",
    "    LANES_FWD = \"lanes:forward\"\n",
    "    LANES_BWD = \"lanes:backward\"\n",
    "    MAXSPEED = \"maxspeed\"\n",
    "    MAXSPEED_FWD = \"maxspeed:forward\"\n",
    "    MAXSPEED_BWD = \"maxspeed:backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T07:56:53.108414Z",
     "start_time": "2025-06-23T07:56:53.099794Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_ways_geometry(file_path):\n",
    "    \"\"\"Load ways CSV with geometry data\"\"\"\n",
    "    ways_df = pd.read_csv(file_path)\n",
    "    ways_df['geometry'] = ways_df['geometry'].apply(loads)\n",
    "    return ways_df[['osmid', 'geometry']]\n",
    "\n",
    "def load_issues_file(issues_csv_path):\n",
    "    \"\"\"Load issues CSV file generated by eval.py\"\"\"\n",
    "    if not os.path.exists(issues_csv_path):\n",
    "        raise FileNotFoundError(f\"Issues file not found: {issues_csv_path}\")\n",
    "    \n",
    "    issues_df = pd.read_csv(issues_csv_path)\n",
    "    print(f\"Loaded {len(issues_df)} issues from {issues_csv_path}\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nIssue Summary:\")\n",
    "    print(issues_df.groupby(['tag', 'issue_type']).size().unstack(fill_value=0))\n",
    "    \n",
    "    return issues_df\n",
    "\n",
    "def filter_issues_by_tag(issues_df, tag):\n",
    "    \"\"\"Filter issues for a specific OSM tag\"\"\"\n",
    "    filtered = issues_df[issues_df['tag'] == tag].copy()\n",
    "    \n",
    "    if len(filtered) == 0:\n",
    "        print(f\"No issues found for tag: {tag}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate metrics for this tag\n",
    "    tp_count = len(filtered[filtered['issue_type'] == 'TP'])\n",
    "    fp_count = len(filtered[filtered['issue_type'].isin(['FP', 'Mismatch'])])\n",
    "    fn_count = len(filtered[filtered['issue_type'].isin(['FN', 'Mismatch'])])\n",
    "    \n",
    "    precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "    recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nMetrics for '{tag}':\")\n",
    "    print(f\"  TP: {tp_count}, FP: {fp_count}, FN: {fn_count}\")\n",
    "    print(f\"  Precision: {precision:.2%}\")\n",
    "    print(f\"  Recall: {recall:.2%}\")\n",
    "    print(f\"  F1 Score: {f1:.2%}\")\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "def create_issue_geodataframe(issues_df, ways_geometry_df):\n",
    "    \"\"\"Merge issues with geometry to create a GeoDataFrame\"\"\"\n",
    "    # Merge issues with geometry\n",
    "    gdf = issues_df.merge(ways_geometry_df, on='osmid', how='left')\n",
    "    \n",
    "    # Remove rows without geometry (if any)\n",
    "    gdf = gdf[gdf['geometry'].notna()]\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(gdf, geometry='geometry', crs='EPSG:4326')\n",
    "    \n",
    "    # Add color mapping\n",
    "    color_map = {\n",
    "        'TP': 'green',\n",
    "        'FP': 'red', \n",
    "        'FN': 'orange',\n",
    "        'Mismatch': 'purple'\n",
    "    }\n",
    "    gdf['color'] = gdf['issue_type'].map(color_map)\n",
    "    \n",
    "    # Create informative popup text\n",
    "    gdf['popup_text'] = gdf.apply(lambda row: \n",
    "        f\"<b>OSMID:</b> {row['osmid']}<br>\"\n",
    "        f\"<b>Tag:</b> {row['tag']}<br>\"\n",
    "        f\"<b>Issue Type:</b> {row['issue_type']}<br>\"\n",
    "        f\"<b>Ground Truth:</b> {row['ground_truth']}<br>\"\n",
    "        f\"<b>Prediction:</b> {row['prediction']}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def plot_issues_map(gdf, title=None):\n",
    "    \"\"\"Plot issues on an interactive map\"\"\"\n",
    "    if len(gdf) == 0:\n",
    "        print(\"No data to plot\")\n",
    "        return None\n",
    "    \n",
    "    m = gdf.explore(\n",
    "        tiles=\"cartodb positron\",\n",
    "        color=gdf['color'],\n",
    "        tooltip='popup_text',\n",
    "        popup=True,\n",
    "        style_kwds={'weight': 7, 'opacity': 0.7}\n",
    "    )\n",
    "    \n",
    "    if title:\n",
    "        print(f\"\\n{title}\")\n",
    "    return m\n",
    "\n",
    "def filter_by_issue_type(gdf, issue_types):\n",
    "    \"\"\"Filter GeoDataFrame by specific issue types\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame with issues\n",
    "        issue_types: str or list of str ('TP', 'FP', 'FN', 'Mismatch')\n",
    "    \n",
    "    Returns:\n",
    "        Filtered GeoDataFrame\n",
    "    \"\"\"\n",
    "    if isinstance(issue_types, str):\n",
    "        issue_types = [issue_types]\n",
    "    \n",
    "    filtered = gdf[gdf['issue_type'].isin(issue_types)].copy()\n",
    "    print(f\"Filtered to {len(filtered)} {'/'.join(issue_types)} issues\")\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the ways geometry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T07:56:53.153713Z",
     "start_time": "2025-06-23T07:56:53.134181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load ways geometry (only needed once)\n",
    "ways_geometry = load_ways_geometry('../metadata/ways.csv')\n",
    "print(f\"Loaded geometry for {len(ways_geometry)} ways\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the issues file (generated by eval.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T07:56:53.170306Z",
     "start_time": "2025-06-23T07:56:53.155592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load issues file from evaluation_results directory\n",
    "# You can change this to any issues file generated by eval.py\n",
    "issues_file = '../evaluation_results/issues_claude_3.5sonnet.csv'\n",
    "\n",
    "issues_df = load_issues_file(issues_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Filter issues by a specific OSM tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T07:56:53.191302Z",
     "start_time": "2025-06-23T07:56:53.171619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose which tag to investigate\n",
    "# Options: 'name', 'oneway', 'lanes', 'lanes:forward', 'lanes:backward', \n",
    "#          'turn:lanes', 'turn:lanes:forward', 'turn:lanes:backward',\n",
    "#          'maxspeed', 'maxspeed:forward', 'maxspeed:backward'\n",
    "\n",
    "tag_to_investigate = 'name'  # Change this to investigate different tags\n",
    "\n",
    "filtered_issues = filter_issues_by_tag(issues_df, tag_to_investigate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create GeoDataFrame with geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filtered_issues is not None:\n",
    "    issues_gdf = create_issue_geodataframe(filtered_issues, ways_geometry)\n",
    "    print(f\"\\nCreated GeoDataFrame with {len(issues_gdf)} issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize on Map\n",
    "\n",
    "**Color Legend:**\n",
    "- ðŸŸ¢ **Green** = True Positive (TP) - Correct prediction\n",
    "- ðŸ”´ **Red** = False Positive (FP) - Predicted when shouldn't have\n",
    "- ðŸŸ  **Orange** = False Negative (FN) - Missed prediction\n",
    "- ðŸŸ£ **Purple** = Mismatch - Both exist but wrong value (counts as both FP and FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all issues for the selected tag\n",
    "if filtered_issues is not None:\n",
    "    plot_issues_map(issues_gdf, title=f\"All Issues for '{tag_to_investigate}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Show only False Negatives (missed predictions)\n",
    "if filtered_issues is not None:\n",
    "    fn_only = filter_by_issue_type(issues_gdf, 'FN')\n",
    "    m = plot_issues_map(fn_only, title=f\"False Negatives for '{tag_to_investigate}'\")\n",
    "    m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Show only False Positives (extra/wrong predictions)\n",
    "if filtered_issues is not None:\n",
    "    fp_only = filter_by_issue_type(issues_gdf, 'FP')\n",
    "    m = plot_issues_map(fp_only, title=f\"False Positives for '{tag_to_investigate}'\")\n",
    "    display(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Show only Mismatches (wrong values)\n",
    "if filtered_issues is not None:\n",
    "    mismatch_only = filter_by_issue_type(issues_gdf, 'Mismatch')\n",
    "    m = plot_issues_map(mismatch_only, title=f\"Mismatches for '{tag_to_investigate}'\")\n",
    "    display(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Show all errors (exclude True Positives)\n",
    "if filtered_issues is not None:\n",
    "    errors_only = filter_by_issue_type(issues_gdf, ['FP', 'FN', 'Mismatch'])\n",
    "    plot_issues_map(errors_only, title=f\"All Errors for '{tag_to_investigate}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Inspect Specific OSMIDs\n",
    "\n",
    "You can look up details for specific ways:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up all issues for a specific OSMID\n",
    "osmid_to_check = 1329887723  # Change this to any osmid you want to investigate\n",
    "\n",
    "osmid_issues = issues_df[issues_df['osmid'] == osmid_to_check]\n",
    "if len(osmid_issues) > 0:\n",
    "    print(f\"\\nIssues for OSMID {osmid_to_check}:\")\n",
    "    print(osmid_issues[['tag', 'issue_type', 'ground_truth', 'prediction']].to_string(index=False))\n",
    "else:\n",
    "    print(f\"No issues found for OSMID {osmid_to_check}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Compare Multiple Models\n",
    "\n",
    "You can load and compare issues from different model evaluations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare metrics from different models\n",
    "import glob\n",
    "\n",
    "# Find all issues files\n",
    "issues_files = glob.glob('../evaluation_results/issues_*.csv')\n",
    "print(f\"Found {len(issues_files)} issues files:\\n\")\n",
    "\n",
    "# Compare metrics for a specific tag across all models\n",
    "comparison_tag = 'name'\n",
    "comparison_results = []\n",
    "\n",
    "for file in issues_files:\n",
    "    model_name = Path(file).stem.replace('issues_', '')\n",
    "    df = pd.read_csv(file)\n",
    "    tag_issues = df[df['tag'] == comparison_tag]\n",
    "    \n",
    "    if len(tag_issues) > 0:\n",
    "        tp = len(tag_issues[tag_issues['issue_type'] == 'TP'])\n",
    "        fp = len(tag_issues[tag_issues['issue_type'].isin(['FP', 'Mismatch'])])\n",
    "        fn = len(tag_issues[tag_issues['issue_type'].isin(['FN', 'Mismatch'])])\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'model': model_name,\n",
    "            'tag': comparison_tag,\n",
    "            'precision': f\"{precision:.2%}\",\n",
    "            'recall': f\"{recall:.2%}\",\n",
    "            'f1': f\"{f1:.2%}\"\n",
    "        })\n",
    "\n",
    "if comparison_results:\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    print(f\"\\nComparison for '{comparison_tag}' tag across models:\")\n",
    "    print(comparison_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
